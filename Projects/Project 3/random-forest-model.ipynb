{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3 Web APIs and NLP\n",
    "\n",
    "_Authors: Joel Quek (SG)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Model to match posts from r/investing, r/stockmarket, r/wallstreetbets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reddit Post Classification](https://towardsdatascience.com/reddit-post-classification-b70258d6affe)\n",
    "\n",
    "[Text Cleaning for NLP: A Tutorial](https://monkeylearn.com/blog/text-cleaning/) [Important]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All libraries used in this project are listed here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, recall_score, precision_score,accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score, plot_roc_curve, roc_auc_score, recall_score, precision_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Scraped Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jupytyer notebooks for scraping are 'reddit-scrape.ipynb' and 'wallstreetbets-scrape.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "investing_df = pd.read_csv('datasets/investing.csv')\n",
    "stockmarket_df = pd.read_csv('datasets/stockmarket.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## r/investing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7995, 75)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investing_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1657271926"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investing_df.iloc[investing_df.shape[0]-1]['created_utc']\n",
    "\n",
    "# GMT: Friday, July 8, 2022 9:18:46 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>investing</td>\n",
       "      <td>HomeInvading</td>\n",
       "      <td>Hey guys, Iâ€™m a 22 year old male, I grew up wi...</td>\n",
       "      <td>Help a young man out would ya?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>investing</td>\n",
       "      <td>ocean-airseashell10</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>Treasury bonds is it a good idea to buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>investing</td>\n",
       "      <td>ocean-airseashell10</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>How to buy treasury bonds? Is treasuryâ€™s direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>investing</td>\n",
       "      <td>iamjokingiamserious</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>Early Exercise of Stock Options</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>investing</td>\n",
       "      <td>jamesterryburke01</td>\n",
       "      <td>Hello Redditors ðŸ‘‹ \\n\\nI work as a Investment C...</td>\n",
       "      <td>Alternative Investments -</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit               author  \\\n",
       "0  investing         HomeInvading   \n",
       "1  investing  ocean-airseashell10   \n",
       "2  investing  ocean-airseashell10   \n",
       "3  investing  iamjokingiamserious   \n",
       "4  investing    jamesterryburke01   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  Hey guys, Iâ€™m a 22 year old male, I grew up wi...   \n",
       "1                                          [removed]   \n",
       "2                                          [removed]   \n",
       "3                                          [removed]   \n",
       "4  Hello Redditors ðŸ‘‹ \\n\\nI work as a Investment C...   \n",
       "\n",
       "                                               title  \n",
       "0                     Help a young man out would ya?  \n",
       "1            Treasury bonds is it a good idea to buy  \n",
       "2  How to buy treasury bonds? Is treasuryâ€™s direc...  \n",
       "3                    Early Exercise of Stock Options  \n",
       "4                          Alternative Investments -  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investing_df=investing_df[['subreddit', 'author', 'selftext', 'title']]\n",
    "investing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## r/stockmarket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7494, 81)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stockmarket_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1657678438"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stockmarket_df.iloc[stockmarket_df.shape[0]-1]['created_utc']\n",
    "\n",
    "# GMT: Wednesday, July 13, 2022 2:13:58 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StockMarket</td>\n",
       "      <td>zitrored</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Looking for the next exogenous event that take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StockMarket</td>\n",
       "      <td>CompetitiveMission1</td>\n",
       "      <td>[Link to the full article (4 min read)](https:...</td>\n",
       "      <td>China stocks notch trillion-dollar gain on hop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>StockMarket</td>\n",
       "      <td>jaltrading21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Get ready for some economic news and company e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>StockMarket</td>\n",
       "      <td>ShabbyShamble</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Market Recap! Bear Market Blues! Palantir (PLT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>StockMarket</td>\n",
       "      <td>PriceActionHelp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Why it's not smart to rely on the RSI divergence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit               author  \\\n",
       "0  StockMarket             zitrored   \n",
       "1  StockMarket  CompetitiveMission1   \n",
       "2  StockMarket         jaltrading21   \n",
       "3  StockMarket        ShabbyShamble   \n",
       "4  StockMarket      PriceActionHelp   \n",
       "\n",
       "                                            selftext  \\\n",
       "0                                                NaN   \n",
       "1  [Link to the full article (4 min read)](https:...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                               title  \n",
       "0  Looking for the next exogenous event that take...  \n",
       "1  China stocks notch trillion-dollar gain on hop...  \n",
       "2  Get ready for some economic news and company e...  \n",
       "3  Market Recap! Bear Market Blues! Palantir (PLT...  \n",
       "4   Why it's not smart to rely on the RSI divergence  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stockmarket_df=stockmarket_df[['subreddit', 'author', 'selftext', 'title']]\n",
    "stockmarket_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "investing_df['selftext']=investing_df['selftext'].fillna('')\n",
    "stockmarket_df['selftext']=stockmarket_df['selftext'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7995 entries, 0 to 7994\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   subreddit  7995 non-null   object\n",
      " 1   author     7995 non-null   object\n",
      " 2   selftext   7995 non-null   object\n",
      " 3   title      7995 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 250.0+ KB\n"
     ]
    }
   ],
   "source": [
    "investing_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7494 entries, 0 to 7493\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   subreddit  7494 non-null   object\n",
      " 1   author     7494 non-null   object\n",
      " 2   selftext   7494 non-null   object\n",
      " 3   title      7494 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 234.3+ KB\n"
     ]
    }
   ],
   "source": [
    "stockmarket_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will combine the text from columns 'author','selftext' and 'title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investing_df['Posts']='Author: '+investing_df['author']+' Title: ' + investing_df['title']+' Text: '+investing_df['selftext']\n",
    "#stockmarket_df['Posts']='Author: '+stockmarket_df['author']+' Title: ' + stockmarket_df['title']+' Text: '+stockmarket_df['selftext']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investing_df['Posts']=investing_df['author']+ ' '+investing_df['title']+' '+investing_df['selftext']\n",
    "stockmarket_df['Posts']=stockmarket_df['author']+ ' '+stockmarket_df['title']+' '+stockmarket_df['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investing_df=investing_df[['subreddit','Posts']]\n",
    "stockmarket_df=stockmarket_df[['subreddit','Posts']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>Posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>investing</td>\n",
       "      <td>HomeInvading Help a young man out would ya? He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>investing</td>\n",
       "      <td>ocean-airseashell10 Treasury bonds is it a goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>investing</td>\n",
       "      <td>ocean-airseashell10 How to buy treasury bonds?...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                              Posts\n",
       "0  investing  HomeInvading Help a young man out would ya? He...\n",
       "1  investing  ocean-airseashell10 Treasury bonds is it a goo...\n",
       "2  investing  ocean-airseashell10 How to buy treasury bonds?..."
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investing_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>Posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StockMarket</td>\n",
       "      <td>zitrored Looking for the next exogenous event ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StockMarket</td>\n",
       "      <td>CompetitiveMission1 China stocks notch trillio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>StockMarket</td>\n",
       "      <td>jaltrading21 Get ready for some economic news ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                              Posts\n",
       "0  StockMarket  zitrored Looking for the next exogenous event ...\n",
       "1  StockMarket  CompetitiveMission1 China stocks notch trillio...\n",
       "2  StockMarket  jaltrading21 Get ready for some economic news ..."
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stockmarket_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate both Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([investing_df,stockmarket_df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15489, 2)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "investing      7995\n",
       "StockMarket    7494\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>Posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>investing</td>\n",
       "      <td>HomeInvading Help a young man out would ya? He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>investing</td>\n",
       "      <td>ocean-airseashell10 Treasury bonds is it a goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>investing</td>\n",
       "      <td>ocean-airseashell10 How to buy treasury bonds?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>investing</td>\n",
       "      <td>iamjokingiamserious Early Exercise of Stock Op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>investing</td>\n",
       "      <td>jamesterryburke01 Alternative Investments - He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                              Posts\n",
       "0  investing  HomeInvading Help a young man out would ya? He...\n",
       "1  investing  ocean-airseashell10 Treasury bonds is it a goo...\n",
       "2  investing  ocean-airseashell10 How to buy treasury bonds?...\n",
       "3  investing  iamjokingiamserious Early Exercise of Stock Op...\n",
       "4  investing  jamesterryburke01 Alternative Investments - He..."
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Posts']=[c.lower() for c in df['Posts']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Non Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Posts']=df['Posts'].map(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    stops = stopwords.words('english')\n",
    "    stops=set(stops)\n",
    "    text = \" \".join([word for word in text.split() if word not in (stops)])\n",
    "    # text = [w for w in text if not w in stops]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Posts']=df['Posts'].map(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hot Encode Target Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subreddit']=df['subreddit'].map({'investing': 0, 'StockMarket': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>Posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>homeinvading help young man would ya hey guys ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ocean airseashell treasury bonds good idea buy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ocean airseashell buy treasury bonds treasury ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>iamjokingiamserious early exercise stock optio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>jamesterryburke alternative investments hello ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                              Posts\n",
       "0          0  homeinvading help young man would ya hey guys ...\n",
       "1          0  ocean airseashell treasury bonds good idea buy...\n",
       "2          0  ocean airseashell buy treasury bonds treasury ...\n",
       "3          0  iamjokingiamserious early exercise stock optio...\n",
       "4          0  jamesterryburke alternative investments hello ..."
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['Posts']\n",
    "y=df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.516173\n",
       "1    0.483827\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y, # stratify means the proportion of 0s and 1s are kept\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CountVectorizer with the default hyperparameters.\n",
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the vectorizer on our corpus.\n",
    "cvec.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the corpus.\n",
    "X_train = cvec.transform(X_train) \n",
    "# transform creates the vector of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10377, 29062)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 492)\t1\n",
      "  (0, 1333)\t1\n",
      "  (0, 5180)\t1\n",
      "  (0, 10765)\t1\n",
      "  (0, 13654)\t1\n",
      "  (0, 14529)\t1\n",
      "  (0, 16149)\t1\n",
      "  (0, 16984)\t1\n",
      "  (0, 25442)\t1\n",
      "  (0, 25662)\t1\n",
      "  (1, 286)\t1\n",
      "  (1, 3116)\t1\n",
      "  (1, 3428)\t1\n",
      "  (1, 12578)\t1\n",
      "  (1, 20803)\t1\n",
      "  (1, 21060)\t1\n",
      "  (2, 358)\t1\n",
      "  (2, 3506)\t1\n",
      "  (2, 3521)\t1\n",
      "  (2, 4505)\t1\n",
      "  (2, 4593)\t1\n",
      "  (2, 6099)\t2\n",
      "  (2, 7480)\t1\n",
      "  (2, 8155)\t1\n",
      "  (2, 8625)\t1\n",
      "  :\t:\n",
      "  (10376, 11061)\t1\n",
      "  (10376, 11282)\t1\n",
      "  (10376, 11283)\t1\n",
      "  (10376, 11787)\t1\n",
      "  (10376, 11897)\t2\n",
      "  (10376, 12933)\t1\n",
      "  (10376, 12953)\t3\n",
      "  (10376, 12968)\t1\n",
      "  (10376, 14538)\t1\n",
      "  (10376, 15784)\t1\n",
      "  (10376, 15999)\t1\n",
      "  (10376, 16307)\t1\n",
      "  (10376, 16868)\t1\n",
      "  (10376, 18821)\t1\n",
      "  (10376, 19351)\t1\n",
      "  (10376, 19365)\t1\n",
      "  (10376, 19585)\t1\n",
      "  (10376, 22157)\t1\n",
      "  (10376, 22451)\t1\n",
      "  (10376, 23042)\t1\n",
      "  (10376, 24014)\t1\n",
      "  (10376, 25441)\t1\n",
      "  (10376, 25507)\t1\n",
      "  (10376, 26096)\t1\n",
      "  (10376, 27092)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['amikuna',\n",
       " 'amilton',\n",
       " 'amit',\n",
       " 'aml',\n",
       " 'amlp',\n",
       " 'amlzf',\n",
       " 'ammakshooter',\n",
       " 'ammo',\n",
       " 'ammonia',\n",
       " 'ammonium']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec.get_feature_names()[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test\n",
    "X_test = cvec.transform(X_test) # we fit the CountVectorizer data based on the X_Train and use that model to transform T_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert training data to dataframe\n",
    "X_train_df = pd.DataFrame(X_train.todense(), \n",
    "                          columns=cvec.get_feature_names()) # cvec is the CountVectorizer object\n",
    "\n",
    "# plot top occuring words\n",
    "X_train_df.sum().sort_values(ascending=False).head(10).plot(kind='barh'); # head(10) gives top ten words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.516236\n",
       "1    0.483764\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinstantiate Train and Test Data [Should I do this for random forest?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid error of instantiating/fitting count vectorizer down below [IMPORTANT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y, # stratify means the proportion of 0s and 1s are kept\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Random Forest with Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. CountVectorizer (transformer)\n",
    "# 2. Random Forest (estimator)\n",
    "\n",
    "pipe1 = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80780347, 0.81647399, 0.79518072, 0.80722892, 0.80385542])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipe1, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.994410716006553\n",
      "0.8073161189358372\n"
     ]
    }
   ],
   "source": [
    "# ii. Fit into model\n",
    "pipe1.fit(X_train, y_train)\n",
    "\n",
    "# Training score\n",
    "print(pipe1.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8073161189358372\n"
     ]
    }
   ],
   "source": [
    "# Test score\n",
    "print(pipe1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Versus Extra Trees [Comparison]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation [Random Forest Versus Extra Trees]\n",
    "---\n",
    "\n",
    "Which one has a higher `cross_val_score`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\redoc\\OneDrive\\Desktop\\DSI-Roughpaper\\0. project_3 (DO THIS)\\random-forest-model.ipynb Cell 73\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/redoc/OneDrive/Desktop/DSI-Roughpaper/0.%20project_3%20%28DO%20THIS%29/random-forest-model.ipynb#Y262sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cross_val_score(rf, X_train_df, y_train, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:476\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    465\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    466\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    467\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    468\u001b[0m ]\n\u001b[0;32m    470\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    477\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    478\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    479\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    480\u001b[0m )(\n\u001b[0;32m    481\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    482\u001b[0m         t,\n\u001b[0;32m    483\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    484\u001b[0m         X,\n\u001b[0;32m    485\u001b[0m         y,\n\u001b[0;32m    486\u001b[0m         sample_weight,\n\u001b[0;32m    487\u001b[0m         i,\n\u001b[0;32m    488\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    489\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    490\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    491\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    492\u001b[0m     )\n\u001b[0;32m    493\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    494\u001b[0m )\n\u001b[0;32m    496\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    187\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 189\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    940\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    970\u001b[0m         X,\n\u001b[0;32m    971\u001b[0m         y,\n\u001b[0;32m    972\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    973\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    975\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    449\u001b[0m         splitter,\n\u001b[0;32m    450\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    456\u001b[0m     )\n\u001b[1;32m--> 458\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    461\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cross_val_score(rf, X_train_df, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(et, X_train_df, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'cvec__max_features': [2_000, 3_000, 4_000, 5_000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [None, 1, 2, 3, 4, 5], # max_depth is the longest path\n",
    "}\n",
    "gs = GridSearchCV(rf, param_grid=rf_params, cv=5)\n",
    "gs.fit(X_train_df, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert testing data to dataframe\n",
    "X_test_df = pd.DataFrame(X_test.todense(), \n",
    "                          columns=cvec.get_feature_names()) # cvec is the CountVectorizer object\n",
    "\n",
    "# plot top occuring words\n",
    "X_test_df.sum().sort_values(ascending=False).head(10).plot(kind='barh'); # head(10) gives top ten words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.score(X_test, y_test) # or X_test_df ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis [Type 1 and Type 2 Errors]\n",
    "\n",
    "https://www.datasciencecentral.com/understanding-type-i-and-type-ii-errors/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score, plot_roc_curve, roc_auc_score, recall_score, precision_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gridsearch Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "preds = gs.predict(X_test)\n",
    "\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix(y_test, preds)\n",
    "cm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View confusion matrix\n",
    "\n",
    "plot_confusion_matrix(gs, X_test, y_test, cmap='Blues', values_format='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the specificity\n",
    "\n",
    "spec = tn / (tn + fp)\n",
    "\n",
    "print('Specificity:', spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sensitivity\n",
    "\n",
    "sens = tp/(tp+fn)\n",
    "\n",
    "print('Sensitivity:', sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis [Type 1 and Type 2 Errors]\n",
    "\n",
    "https://www.datasciencecentral.com/understanding-type-i-and-type-ii-errors/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many Type I errors [FALSE POSITIVE] are there?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many Type II errors [FALSE NEGATIVE] are there?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which error is worse (Type I vs Type II)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(gs, X_test, y_test)\n",
    "plt.plot([0, 1], [0, 1],\n",
    "         label='baseline', linestyle='--')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area under the ROC curve measures the degree of overlap between the Specificity and Sensitivity distributions. Greater the area, the less overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Random Forest with TFID Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. TFIDVectorizer (transformer)\n",
    "# 2. Random Forest (estimator)\n",
    "\n",
    "pipe2 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipe2, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii. Fit into model\n",
    "pipe2.fit(X_train, y_train)\n",
    "\n",
    "# Training score\n",
    "print(pipe2.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'tvec__max_features': [2_000, 3_000, 4_000, 5_000],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)] # what about min_df and max_df?\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [None, 1, 2, 3, 4, 5], # max_depth is the longest path\n",
    "}\n",
    "gs2 = GridSearchCV(rf, param_grid=rf_params, cv=5)\n",
    "gs2.fit(X_train_df, y_train)\n",
    "print(gs2.best_score_)\n",
    "gs2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert testing data to dataframe\n",
    "X_test_df = pd.DataFrame(X_test.todense(), \n",
    "                          columns=cvec.get_feature_names()) # cvec is the CountVectorizer object\n",
    "\n",
    "# plot top occuring words\n",
    "X_test_df.sum().sort_values(ascending=False).head(10).plot(kind='barh'); # head(10) gives top ten words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2.score(X_test, y_test) # or X_test_df ? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1131d84b9e97d700f196cec3f143c1c5ca4787d89ba01101505d30befb8a4c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
